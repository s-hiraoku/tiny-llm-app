import * as ort from "onnxruntime-web";
import { AutoTokenizer } from "@xenova/transformers";

interface InferenceInput {
  question: string;
  context: string;
}

interface InferenceResult {
  answer: string;
  score: number;
}

/**
 * è³ªå•å¿œç­”ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
 */
export async function performQAInference({
  question,
  context,
  modelPath = "/qa-model.onnx",
  modelName = "ybelkada/japanese-roberta-question-answering",
  maxLength = 512,
}: InferenceInput & {
  modelPath?: string;
  modelName?: string;
  maxLength?: number;
}): Promise<InferenceResult> {
  try {
    // ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
    const tokenizer = await AutoTokenizer.from_pretrained(modelName);

    // å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    const tokens = await tokenizer(question, context, {
      padding: true,
      truncation: true,
      maxLength: maxLength,
      returnTensors: true,
    });

    // ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    const inputIds = Array.from(tokens.input_ids.data);
    const attentionMask = Array.from(tokens.attention_mask.data);

    // ONNXãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
    const session = await ort.InferenceSession.create(modelPath);

    // ONNX Runtimeã§ä½¿ç”¨ã™ã‚‹å…¥åŠ›å½¢å¼ã«å¤‰æ›
    const feeds = {
      input_ids: new ort.Tensor("int64", inputIds, [1, inputIds.length]),
      attention_mask: new ort.Tensor("int64", attentionMask, [
        1,
        attentionMask.length,
      ]),
    };

    // æ¨è«–ã®å®Ÿè¡Œ
    const output = await session.run(feeds);

    // é–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’å–å¾—
    const startLogits = Object.values(output.start_logits.cpuData);
    const endLogits = Object.values(output.end_logits.cpuData);

    // ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ç¢ºç‡ã‚’è¨ˆç®—
    function softmax(logits: number[]) {
      const max = Math.max(...logits);
      const exp = logits.map((x) => Math.exp(x - max));
      const sum = exp.reduce((a, b) => a + b, 0);
      return exp.map((x) => x / sum);
    }

    const softmaxStartLogits = softmax(startLogits);
    const softmaxEndLogits = softmax(endLogits);

    let startPosition = softmaxStartLogits.indexOf(
      Math.max(...softmaxStartLogits)
    );
    let endPosition = softmaxEndLogits.indexOf(Math.max(...softmaxEndLogits));

    // æœ‰åŠ¹ãªé–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã®ç¢ºèª
    if (
      startPosition === -1 ||
      endPosition === -1 ||
      startPosition >= inputIds.length ||
      endPosition >= inputIds.length ||
      startPosition >= endPosition
    ) {
      // ã‚ˆã‚Šè©³ç´°ãªè¨ºæ–­æƒ…å ±ã‚’å‡ºåŠ›
      console.log("Diagnostic Information:", {
        startLogits,
        endLogits,
        softmaxStartLogits,
        softmaxEndLogits,
        startPosition,
        endPosition,
        inputIdsLength: inputIds.length,
      });

      // ãƒˆãƒƒãƒ—5ã®å€™è£œä½ç½®ã‚’æ¤œå‡º
      const topStartPositions = softmaxStartLogits
        .map((prob, index) => ({ prob, index }))
        .sort((a, b) => b.prob - a.prob)
        .slice(0, 5);

      const topEndPositions = softmaxEndLogits
        .map((prob, index) => ({ prob, index }))
        .sort((a, b) => b.prob - a.prob)
        .slice(0, 5);

      console.log("Top Start Positions:", topStartPositions);
      console.log("Top End Positions:", topEndPositions);

      // ã‚ˆã‚Šè³¢æ˜ãªä½ç½®é¸æŠ
      const validStartPositions = topStartPositions.filter(
        (pos) => pos.index < inputIds.length
      );
      const validEndPositions = topEndPositions.filter(
        (pos) =>
          pos.index < inputIds.length &&
          pos.index > validStartPositions[0]?.index
      );

      if (validStartPositions.length > 0 && validEndPositions.length > 0) {
        startPosition = validStartPositions[0].index;
        endPosition = validEndPositions[0].index;
      } else {
        return {
          answer: "å›ç­”ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ",
          score: 0,
        };
      }
    }

    // ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¦å›ç­”ã‚’æŠ½å‡º
    const answer_tokens = inputIds.slice(startPosition, endPosition + 1);

    // answer_tokensãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
    if (answer_tokens.length === 0) {
      console.log("ğŸš€ ~ answer_tokens:", answer_tokens);
      return {
        answer: "å›ç­”ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ",
        score: 0,
      };
    }

    const answer = await tokenizer.decode(answer_tokens);

    // const answer = await tokenizer.decode(answer_tokens, {
    //   skip_special_tokens: true,
    //   clean_up_tokenization_spaces: false,
    // });

    // const answer = await tokenizer.decode(answer_tokens, {
    //   skip_special_tokens: true,
    //   clean_up_tokenization_spaces: true,
    //   group_tokens: true,
    // });

    console.log("Input IDs:", inputIds);
    console.log("Start Logits:", startLogits);
    console.log("End Logits:", endLogits);
    console.log("Start Position:", startPosition);
    console.log("End Position:", endPosition);
    console.log("Answer Tokens:", answer_tokens);

    // ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆé–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã®ç¢ºç‡ã®å¹³å‡ï¼‰
    const score = (startLogits[startPosition] + endLogits[endPosition]) / 2;

    // ç©ºã®å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯
    if (!answer || answer.trim() === "") {
      return {
        answer: "å›ç­”ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ",
        score: 0,
      };
    }

    return {
      answer,
      score,
    };
  } catch (error) {
    console.error("æ¨è«–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:", error);
    throw error;
  }
}
